{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+cVf1Kbo/ZTGOiNm7CTPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyrankhademi/CIPA2019_Workshop/blob/master/CIFAR10_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fe37K5CtO0g",
        "colab_type": "text"
      },
      "source": [
        "#Reproducing CIFAR10 Experiment in the ResNet paper\n",
        "In this notebook we reproduce Table 6 in [1] , i.e. the CIFAR10 experiment in the original ResNet paper published in CVPR 2016 conference and received more than 38k citation so far. This Pytorch implementation is based on adaptation of the code in [2] and [3] to Jupyter notebook. \n",
        "\n",
        "## How/Why ResNet Models are Working?\n",
        "There has been rigorous attemps to make deeper convolutional neural networks (CNN) since their advent in 2012 [4] as the performance is believed to be tightly related to the complexity of the network [5]. A major obstacle in training deeper neural networks is the well-known \"vanishing gradient\" problem. As the layers are added to the network the multiplying gradients makes the overall gradient infinitesimal which in turn causes very slow convergence if at all [6]. The same training difficulties with \"exploding gradients\" can also hinder the learning process once layers are stacked in the neural networks. In 2015, a novel CNN architecture is introduced, which won the ImageNet classification competition (ILSVRC 2015) by a good margin (2.84 %) from its competitor [7]. \n",
        "\n",
        "The intuition behind the ResNet architecture is rather simple: Assuming that a neural network unit can learn any function, asymptotically, then it can learn the identity function as well. An example residual unit is shown in the following figure, taken from [1]. The input to the residual block is $X$ and the output is $\\mathcal{F}(X)+X$, therefore, by learning $\\mathcal{F}(X)=0$ this basic block is bypassed during the training process, which is equivalent to identity mapping. A cascade of these residual blocks are used to create very deep CNN models with more than 100 layers as presented in [1].  \n",
        "\n",
        "![](https://drive.google.com/uc?id=1c4QvJN4H_GdGWNM-vW46j_JIG64CD_mD)\n",
        "\n",
        "Note that a plain CNN model (without residual connections) posses the same solution space as the counterpart network with the residual connections, however it is argued in [1] that \"If the optimal function is closer to an identity\n",
        "mapping than to a zero mapping, it should be easier for the\n",
        "solver to find the perturbations with reference to an identity\n",
        "mapping, than to learn the function as a new one.\" This hypothesis is backed up the paper with several experiments in different datasets. \n",
        "\n",
        "## Experiment Setup \n",
        "The authors train and test six different ResNet architectures for CIFAR10 dataset [8] and compare the results on Table 6 in the original paper. CIFAR10 image classification dataset consist of 50k training images and 10k testing\n",
        "images in 10 classes. The network inputs are $32\u0002\\times 32$ images, with\n",
        "the per-pixel mean subtracted. The first layer is $3\u0002\\times 3$ convolutions.\n",
        "There are totally $6n+2$ residual blocks stacked, where replacing $n$ with $3,5,7,9,18,200$ produces networks of depth $20,32,44,56,110,1202$, respectively. The architecture is summerized in the following table taken from the paper, where three columns represent three different feature-map size. \n",
        "\n",
        "![](https://drive.google.com/uc?id=1W_k5HZ8lS9_h9BUOx9R0AvZPTqRVDSlT)\n",
        "\n",
        "Note that training in the original paper uses validation set to select the best performing model, however, this implementation bades on [3] does not do any validation testing. Also to train the ResNet1202, you need 16GB memory on GPU, therefore, you can not run it on Google CoLab platform.\n",
        "For the rest,we use the same setting as described in the original paper and we reproduce the following results in terms of performance error: \n",
        "\n",
        "| MODEL | PAPER   | OURS\n",
        "|------|------||\n",
        "|   ResNet-20  | 8.75|7.92|\n",
        "|   ResNet-32  |7.51 |7.27|\n",
        "|   ResNet-44  |7.17|6.88|\n",
        "|   ResNet-56  |6.97 |7.58|\n",
        "|   ResNet-110  | 6.43|9.04|\n",
        "|   ResNet-1202  | 7.93|?|\n",
        "\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, \"Deep Residual Learning for Image Recognition\". CVPR 2016\n",
        "\n",
        "[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "\n",
        "[3] https://github.com/akamaster/pytorch_resnet_cifar10\n",
        "\n",
        "[4] Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, \"ImageNet Classification with Deep Convolutional Neural Networks\", NIPS 2012\n",
        "\n",
        "[5] K. Simonyan and A. Zisserman,\"Very deep convolutional networks\n",
        "for large-scale image recognition\" ICLR 2015\n",
        "\n",
        "[6] X. Glorot and Y. Bengio, \"Understanding the difficulty of training\n",
        "deep feedforward neural networks\", AISTATS 2010\n",
        "\n",
        "[7] K. He, X. Zhang, S. Ren, and J. Sun, \"Delving deep into rectifiers:\n",
        "Surpassing human-level performance on imagenet classification\", ICCV 2015\n",
        "\n",
        "[8] https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAEdiF4Ymizi",
        "colab_type": "text"
      },
      "source": [
        "## Running the Experiment on Google Colab\n",
        "This notebook is running remotly on Google Colab platform, therefore to save and access the trained model and chekpoints in your local computer you may need to mounth the Google drive (gdrive).  The following code snippet will setup a local drive in your computer. Then, you can create a directory in your gdrive for this project and specify the path to this directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H418uCYl6OlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First we need to mount the Google drive and specify the path to the directory\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls \"/content/gdrive/My Drive/CIFAR10_ResNet\" \n",
        "root_path = 'gdrive/My Drive/CIFAR10_ResNet' \n",
        "path ='/content/gdrive/My Drive/CIFAR10_ResNet'\n",
        "os.chdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjvbl18HsjnT",
        "colab_type": "text"
      },
      "source": [
        "In the following code cell, the ResNet model is defined with all the related functions for initialization of the weights, the residual block and skip connections as presented in the original paper. By executing the cell you can see all possible ResNet architecture, with the number of their learning parameters and layers, that we use in this experiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Y2hYRwB-qg",
        "colab_type": "code",
        "outputId": "579c6c04-f368-41cc-fc13-4f1748d52376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import resnet\n",
        "# We define all the classes and function regarding the ResNet architecture in this code cell\n",
        "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
        " \n",
        "def _weights_init(m):\n",
        "    \"\"\"\n",
        "        Initialization of CNN weights\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    \"\"\"\n",
        "      Identity mapping between ResNet blocks with diffrenet size feature map\n",
        "    \"\"\"\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "# A basic block as shown in Fig.3 (right) in the paper consists of two convolutional blocks, each followed by a Bach-Norm layer. \n",
        "# Every basic block is shortcuted in ResNet architecture to construct f(x)+x module. \n",
        "# Expansion for option 'A' in the paper is equal to identity with extra zero entries padded\n",
        "# for increasing dimensions between layers with different feature map size. This option introduces no extra parameter. \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 experiment, ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# Stack of 3 times 2*n (n is the number of basic blocks) layers are used for making the ResNet model, \n",
        "# where each 2n layers have feature maps of size {16,32,64}, respectively. \n",
        "# The subsampling is performed by convolutions with a stride of 2.\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "\n",
        "def resnet56():\n",
        "    return ResNet(BasicBlock, [9, 9, 9])\n",
        "\n",
        "\n",
        "def resnet110():\n",
        "    return ResNet(BasicBlock, [18, 18, 18])\n",
        "\n",
        "\n",
        "def resnet1202():\n",
        "    return ResNet(BasicBlock, [200, 200, 200])\n",
        "\n",
        "\n",
        "def test(net):\n",
        "    import numpy as np\n",
        "    total_params = 0\n",
        "\n",
        "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
        "        total_params += np.prod(x.data.numpy().shape)\n",
        "    print(\"Total number of params\", total_params)\n",
        "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for net_name in __all__:\n",
        "        if net_name.startswith('resnet'):\n",
        "            print(net_name)\n",
        "            test(globals()[net_name]())\n",
        "            print()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resnet20\n",
            "Total number of params 269722\n",
            "Total layers 20\n",
            "\n",
            "resnet32\n",
            "Total number of params 464154\n",
            "Total layers 32\n",
            "\n",
            "resnet44\n",
            "Total number of params 658586\n",
            "Total layers 44\n",
            "\n",
            "resnet56\n",
            "Total number of params 853018\n",
            "Total layers 56\n",
            "\n",
            "resnet110\n",
            "Total number of params 1727962\n",
            "Total layers 110\n",
            "\n",
            "resnet1202\n",
            "Total number of params 19421274\n",
            "Total layers 1202\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANRMm1zq2J7a",
        "colab_type": "text"
      },
      "source": [
        "We define a class (MyResNetArgs) in the following to assign the hyperparameters such as number of training epochs, learning rate, momentum, batch size, etc. to the training function. The objects of this class are initialized inherently once created with void argument. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIXmCsEZ6dFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class MyResNetArgs:\n",
        "   \"\"\"\n",
        "    Passing the hyperparameters to the model\n",
        "   \"\"\"\n",
        "   def __init__(self, arch='resnet20' ,epochs=200, start_epoch=0, batch_size=128, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=55,\n",
        "                 evaluate=0, half=0, save_dir='save_temp', save_every=10):\n",
        "        self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
        "        self.save_dir = save_dir #The directory used to save the trained models\n",
        "        self.half = half #use half-precision(16-bit)\n",
        "        self.evaluate = evaluate #evaluate model on validation set\n",
        "        self.print_freq = print_freq #print frequency \n",
        "        self.weight_decay = weight_decay\n",
        "        self.momentum = momentum \n",
        "        self.lr = lr #Learning rate\n",
        "        self.batch_size = batch_size \n",
        "        self.start_epoch = start_epoch\n",
        "        self.epochs = epochs\n",
        "        self.arch = arch #ResNet model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpNzFe-13pWj",
        "colab_type": "text"
      },
      "source": [
        "Now we can create an instance of ResNet model and inspect the architecture by printing the model summery. \n",
        "One can easily check the difference between different ResNet models to understand the constructing units. There are totally $6n+2$ stacked weighted layers, e.g., for ResNet 20, there are 19 convolutional layers plus one fully connected layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djW0RO80_prY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "args=MyResNetArgs('resnet20',evaluate=0,epochs=200)\n",
        "#model = resnet.__dict__[args.arch]()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "model = resnet.__dict__[args.arch]().to(device)\n",
        "summary(model, (3,32,32))\n",
        "best_prec1 = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAb3r6OQ43ve",
        "colab_type": "text"
      },
      "source": [
        "The next two code blocks are for training and testing the model on the validation set. There is a print module at the end of train function which prints the top-k classification accuracy and error at specified epochs which is set in \"print_freq\" hyper-parameter. The checkpoints are also saved for the training model at \"save_every\" epoch steps, which is initialized to every 10 epochs by default.  The average accuracy among mini-batches are also recorded for inspection purposes, which is calculated using the \"AverageMeter\" function. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTjp-tkWtmmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "        Run one train epoch\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "        if args.half:\n",
        "            input_var = input_var.half()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B57y5hgMtzDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"\n",
        "    Run evaluation\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            if args.half:\n",
        "                input_var = input_var.half()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "\n",
        "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
        "          .format(top1=top1,error=100-top1.avg))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "def save_checkpoint(state, filename='checkpoint.th'):\n",
        "    \"\"\"\n",
        "    Save the training model\n",
        "    \"\"\"\n",
        "    torch.save(state, filename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKt6Q9Zlt8MU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OKFeRNm7mJA",
        "colab_type": "text"
      },
      "source": [
        "Here comes the main script using all the functions explained above to execute the train and the test procedure for a specified ResNet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONRZVaZtP0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import time\n",
        "\n",
        "def main():\n",
        "    global args, best_prec1\n",
        "    \n",
        "    # Check the save_dir exists or not\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    model = resnet.__dict__[args.arch]()\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]), download=True),\n",
        "        batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])),\n",
        "        batch_size=128, shuffle=False,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "    # define loss function (criterion) and pptimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    if args.half:\n",
        "        print('half persicion is used.')\n",
        "        model.half()\n",
        "        criterion.half()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay)\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
        "\n",
        "    if args.arch in ['resnet1202', 'resnet110']:\n",
        "        # for resnet1202 original paper uses lr=0.01 for first 400 minibatches for warm-up\n",
        "        # then switch back. In this setup it will correspond for first epoch.\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = args.lr*0.1\n",
        "\n",
        "\n",
        "    if args.evaluate:\n",
        "        print('evalution mode')\n",
        "        model.load_state_dict(torch.load(os.path.join(args.save_dir, 'model.th')))\n",
        "        best_prec1 = validate(val_loader, model, criterion)\n",
        "        return best_prec1\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        print('Training {} model'.format(args.arch))\n",
        "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # evaluate on validation set\n",
        "        prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = prec1 > best_prec1\n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "\n",
        "        if epoch > 0 and epoch % args.save_every == 0:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'checkpoint.th'))\n",
        "        if is_best:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'model.th'))\n",
        "\n",
        "    return best_prec1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su50THmTmXMh",
        "colab_type": "text"
      },
      "source": [
        "Since, Google Colab imposes time restriction, we can not loop over all ResNet models to reproduce the results in one go, instead, we run each ResNet model seperatly by setting the name of the model (e.g. resnet20)  manually in the hyperparameters and record the results once training is finished for the specified network. We saved the trained models in a seperate directory to do the inference later. One can try to run the main script several times for each network and report the mean and the variance performance for more reliable results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkntiMU2uKk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "   best_prec1 = main()\n",
        "   print('The lowest error from {} model after {} epochs is {error:.3f}'.format(args.arch,args.epochs,error=100-best_prec1)) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}